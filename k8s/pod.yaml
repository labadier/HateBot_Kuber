apiVersion: v1
kind: Pod
metadata:
  name: hatebot-api
  namespace: hatebot
  
spec:
  containers:
      # basic container details
    - name: mlflow
      image: ghcr.io/mlflow/mlflow
      # imagePullPolicy: Never # always pull the image, even if it is already present on the node
      # hardcode the listening port if Dockerfile isn't set with EXPOSE
      ports:
        - containerPort: 8080
          protocol: TCP
      # environment variables
      env:
        - name: MLFLOW_TRACKING_URI
          value: http://mlflow:8080
      command: ["mlflow", "server", "--backend-store-uri", "sqlite:///mlflow/mlruns/mlflow.db", "--default-artifact-root", "/mlflow/mlruns", "--host", "0.0.0.0", "--port", "8080"]  # The port where MLflow server will be exposed
      # ["ls", "-la" ,"/mlflow/mlruns"]
      volumeMounts:
        - name: mlflow-volume
          mountPath: /mlflow/mlruns # this is the container path, not the host path

    - name: inference
      image: labadier/hatespeech-detection-inference_kube:latest
      ports:
        - containerPort: 8082
          protocol: TCP
      env:
        - name: MLFLOW_TRACKING_URI
          value: http://localhost:8080
        - name: MODEL_NAME
          value: OffenseBERT
        - name: PYTHON_PORT
          value: "8082"
      volumeMounts:
        - name: mlflow-volume
          mountPath: /mlflow/mlruns # this is the container path, not the host path
      command: ["python", "app.py"]  # The port where MLflow server will be exposed
    
      # readinessProbe:        # I always recommend using these, even if your app has no listening ports (this affects any rolling update)
      #   httpGet:             # Lots of timeout values with defaults, be sure they are ideal for your workload
      #     path: http://mlflow/
      #     port: 8080
      # livenessProbe:         # only needed if your app tends to go unresponsive or you don't have a readinessProbe, but this is up for debate
      #   httpGet:             # Lots of timeout values with defaults, be sure they are ideal for your workload
      #     path: http://mlflow/
      #     port: 8080

      # resources:             # Because if limits = requests then QoS is set to "Guaranteed"
      #   limits:
      #     memory: "1000Mi"    # If container uses over 500MB it is killed (OOM)
      #     cpu: "2"          # Not normally needed, unless you need to protect other workloads or QoS must be "Guaranteed"
      #   requests:
      #     memory: "500Mi"    # Scheduler finds a node where 500MB is available
      #     cpu: "1"           # Scheduler finds a node where 1 vCPU is available

      # per-container security context
      # lock down privileges inside the container
      # securityContext:
      #   allowPrivilegeEscalation: false # prevent sudo, etc.
      #   privileged: false               # prevent acting like host root


  # terminationGracePeriodSeconds: 600 # default is 30, but you may need more time to gracefully shutdown (HTTP long polling, user uploads, etc)

  volumes:
    - name: mlflow-volume
      hostPath: 
        path: /mnt/mlflow/mlruns # this is the host path, not the container path
        type: "" # this is the type of volume, not the path

  # per-pod security context
  # enable seccomp and force non-root user
  securityContext:

    seccompProfile:
      type: RuntimeDefault   # enable seccomp and the runtimes default profile

    runAsUser: 501          # hardcode user to non-root if not set in Dockerfile
    runAsGroup: 20         # hardcode group to non-root if not set in Dockerfile
    runAsNonRoot: true       # hardcode to non-root. Redundant to above if Dockerfile is set USER 1000