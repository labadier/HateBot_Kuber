apiVersion: apps/v1
kind: Deployment
metadata:
  name: inference-server
  namespace: hatebot
  
spec:
  replicas: 2
  selector:
    matchLabels:
      app: inference-server-pod
  template:
    metadata:
      labels:
        app: inference-server-pod
      annotations:
        gke-gcsfuse/volumes: "true"
    spec:
      containers:
          # basic container details
        - name: inference
          image: labadier/hatespeech-detection-inference_kube:v1
          ports:
            - containerPort: 8082
              protocol: TCP
          env:
            - name: MLFLOW_TRACKING_URI
              value: http://mlflow-service:8080
            - name: MODEL_NAME
              value: OffenseBERT
            - name: PYTHON_PORT
              value: "8082"
          volumeMounts:
            - name: mlflow-volume
              mountPath: /mlflow
              readOnly: false
          command: ["python", "app.py"]  
          # command: ["/bin/sh","-c"]
          # args:
          #   - |
          #     exec python -c 
          #     sleep 30;
        
      # readinessProbe:        # I always recommend using these, even if your app has no listening ports (this affects any rolling update)
      #   httpGet:             # Lots of timeout values with defaults, be sure they are ideal for your workload
      #     path: http://mlflow/
      #     port: 8080
      # livenessProbe:         # only needed if your app tends to go unresponsive or you don't have a readinessProbe, but this is up for debate
      #   httpGet:             # Lots of timeout values with defaults, be sure they are ideal for your workload
      #     path: http://mlflow/
      #     port: 8080

      # resources:             # Because if limits = requests then QoS is set to "Guaranteed"
      #   limits:
      #     memory: "1000Mi"    # If container uses over 500MB it is killed (OOM)
      #     cpu: "2"          # Not normally needed, unless you need to protect other workloads or QoS must be "Guaranteed"
      #   requests:
      #     memory: "500Mi"    # Scheduler finds a node where 500MB is available
      #     cpu: "1"           # Scheduler finds a node where 1 vCPU is available

      # per-container security context
      # lock down privileges inside the container
      # securityContext:
      #   allowPrivilegeEscalation: false # prevent sudo, etc.
      #   privileged: false               # prevent acting like host root


  # terminationGracePeriodSeconds: 600 # default is 30, but you may need more time to gracefully shutdown (HTTP long polling, user uploads, etc)
      serviceAccountName: hatebot-service-account
      volumes:
        - name: mlflow-volume
          csi:
            driver: gcsfuse.csi.storage.gke.io
            volumeAttributes:
              bucketName: mlruns-artifacts
              mountOptions: "implicit-dirs"
              objectPrefix: mlruns
          # hostPath: 
          #   path: /mnt/mlflow/mlruns # this is the host path, not the container path
          #   type: "" # this is the type of volume, not the path

      # per-pod security context
      # enable seccomp and force non-root user
      securityContext: {}

        # seccompProfile:
        #   type: RuntimeDefault   # enable seccomp and the runtimes default profile

        # runAsUser: 501          # hardcode user to non-root if not set in Dockerfile
        # runAsGroup: 20         # hardcode group to non-root if not set in Dockerfile
        # runAsNonRoot: true       # hardcode to non-root. Redundant to above if Dockerfile is set USER 1000